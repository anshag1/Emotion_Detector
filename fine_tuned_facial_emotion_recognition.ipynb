{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Fine-Tuned Facial Emotion Recognition with Transfer Learning (VGG16)\n","Uses transfer learning, advanced augmentation, and best practices for improved FER2013 accuracy.\n","\n","**Steps:** Install packages, upscale images to 224x224 and convert grayscale to RGB, use VGG16, train with callbacks, save best model, and test in real time."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 1. Install required libraries\n","!pip install tensorflow keras opencv-python numpy matplotlib pandas scikit-learn --quiet\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n","TensorFlow version: 2.15.0\n"]}],"source":["# 2. Imports\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input, Flatten\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","import numpy as np, cv2, os, matplotlib.pyplot as plt, datetime\n","print('TensorFlow version:', tf.__version__)\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 22968 images belonging to 7 classes.\n","Found 5741 images belonging to 7 classes.\n"]}],"source":["batch_size = 64\n","def gray2rgb_resize(img):\n","    import cv2\n","    # img is 2D (h, w, 1)\n","    if img.shape[-1] == 1:\n","        img = img.squeeze(-1)\n","        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n","    img = cv2.resize(img, (224, 224))\n","    return img\n","\n","train_aug = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n","    zoom_range=0.2, shear_range=0.2, horizontal_flip=True, validation_split=0.2,\n","    preprocessing_function=gray2rgb_resize\n",")\n","\n","train_gen = train_aug.flow_from_directory(\n","    'data/train',\n","    target_size=(224, 224),   # Change from (48,48) to (224,224)\n","    color_mode='rgb',         # Change from 'grayscale' to 'rgb'\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    shuffle=True,\n","    subset='training'\n",")\n","\n","val_gen = train_aug.flow_from_directory(\n","    'data/train',\n","    target_size=(224, 224),   # Change here as well\n","    color_mode='rgb',         # Change here as well\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    shuffle=True,\n","    subset='validation'\n",")\n","\n","steps_train = train_gen.samples // batch_size\n","steps_val = val_gen.samples // batch_size\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n","\n","WARNING:tensorflow:From c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," global_average_pooling2d (  (None, 512)               0         \n"," GlobalAveragePooling2D)                                         \n","                                                                 \n"," batch_normalization (Batch  (None, 512)               2048      \n"," Normalization)                                                  \n","                                                                 \n"," dense (Dense)               (None, 512)               262656    \n","                                                                 \n"," dropout (Dropout)           (None, 512)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 7)                 3591      \n","                                                                 \n","=================================================================\n","Total params: 14982983 (57.16 MB)\n","Trainable params: 7346695 (28.03 MB)\n","Non-trainable params: 7636288 (29.13 MB)\n","_________________________________________________________________\n"]}],"source":["# 4. Load pre-trained VGG16 for transfer learning\n","vgg = VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))\n","for layer in vgg.layers[:-4]: layer.trainable = False  # fine-tune only last 4 conv layers\n","x = vgg.output\n","x = GlobalAveragePooling2D()(x)\n","x = BatchNormalization()(x)\n","x = Dense(512, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","preds = Dense(train_gen.num_classes, activation='softmax')(x)\n","model = Model(inputs=vgg.input, outputs=preds)\n","model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","WARNING:tensorflow:From c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n","\n","WARNING:tensorflow:From c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n","\n","358/358 [==============================] - ETA: 0s - loss: 1.6270 - accuracy: 0.3686\n","Epoch 1: val_loss improved from inf to 1.51889, saving model to best_emotion_vgg16.h5\n","358/358 [==============================] - 1796s 5s/step - loss: 1.6270 - accuracy: 0.3686 - val_loss: 1.5189 - val_accuracy: 0.4055 - lr: 1.0000e-04\n","Epoch 2/30\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Program Files\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["358/358 [==============================] - ETA: 0s - loss: 1.3464 - accuracy: 0.4859\n","Epoch 2: val_loss improved from 1.51889 to 1.31844, saving model to best_emotion_vgg16.h5\n","358/358 [==============================] - 1653s 5s/step - loss: 1.3464 - accuracy: 0.4859 - val_loss: 1.3184 - val_accuracy: 0.4921 - lr: 1.0000e-04\n","Epoch 3/30\n","358/358 [==============================] - ETA: 0s - loss: 1.2469 - accuracy: 0.5290\n","Epoch 3: val_loss improved from 1.31844 to 1.26802, saving model to best_emotion_vgg16.h5\n","358/358 [==============================] - 1647s 5s/step - loss: 1.2469 - accuracy: 0.5290 - val_loss: 1.2680 - val_accuracy: 0.5200 - lr: 1.0000e-04\n","Epoch 4/30\n","358/358 [==============================] - ETA: 0s - loss: 1.1865 - accuracy: 0.5521\n","Epoch 4: val_loss improved from 1.26802 to 1.24708, saving model to best_emotion_vgg16.h5\n","358/358 [==============================] - 1656s 5s/step - loss: 1.1865 - accuracy: 0.5521 - val_loss: 1.2471 - val_accuracy: 0.5195 - lr: 1.0000e-04\n","Epoch 5/30\n","358/358 [==============================] - ETA: 0s - loss: 1.1379 - accuracy: 0.5709\n","Epoch 5: val_loss improved from 1.24708 to 1.16196, saving model to best_emotion_vgg16.h5\n","358/358 [==============================] - 1656s 5s/step - loss: 1.1379 - accuracy: 0.5709 - val_loss: 1.1620 - val_accuracy: 0.5592 - lr: 1.0000e-04\n","Epoch 6/30\n","358/358 [==============================] - ETA: 0s - loss: 1.1041 - accuracy: 0.5848\n","Epoch 6: val_loss did not improve from 1.16196\n","358/358 [==============================] - 1653s 5s/step - loss: 1.1041 - accuracy: 0.5848 - val_loss: 1.1819 - val_accuracy: 0.5537 - lr: 1.0000e-04\n","Epoch 7/30\n","358/358 [==============================] - ETA: 0s - loss: 1.0616 - accuracy: 0.6031\n","Epoch 7: val_loss did not improve from 1.16196\n","358/358 [==============================] - 1653s 5s/step - loss: 1.0616 - accuracy: 0.6031 - val_loss: 1.1715 - val_accuracy: 0.5523 - lr: 1.0000e-04\n","Epoch 8/30\n","358/358 [==============================] - ETA: 0s - loss: 1.0364 - accuracy: 0.6100\n","Epoch 8: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n","\n","Epoch 8: val_loss did not improve from 1.16196\n","358/358 [==============================] - 1651s 5s/step - loss: 1.0364 - accuracy: 0.6100 - val_loss: 1.1751 - val_accuracy: 0.5611 - lr: 1.0000e-04\n","Epoch 9/30\n","358/358 [==============================] - ETA: 0s - loss: 0.9656 - accuracy: 0.6422\n","Epoch 9: val_loss improved from 1.16196 to 1.06702, saving model to best_emotion_vgg16.h5\n","358/358 [==============================] - 1655s 5s/step - loss: 0.9656 - accuracy: 0.6422 - val_loss: 1.0670 - val_accuracy: 0.6009 - lr: 2.0000e-05\n","Epoch 10/30\n","358/358 [==============================] - ETA: 0s - loss: 0.9361 - accuracy: 0.6511\n","Epoch 10: val_loss improved from 1.06702 to 1.05862, saving model to best_emotion_vgg16.h5\n","358/358 [==============================] - 1649s 5s/step - loss: 0.9361 - accuracy: 0.6511 - val_loss: 1.0586 - val_accuracy: 0.5983 - lr: 2.0000e-05\n","Epoch 11/30\n","358/358 [==============================] - ETA: 0s - loss: 0.9283 - accuracy: 0.6558\n","Epoch 11: val_loss improved from 1.05862 to 1.04276, saving model to best_emotion_vgg16.h5\n","358/358 [==============================] - 1649s 5s/step - loss: 0.9283 - accuracy: 0.6558 - val_loss: 1.0428 - val_accuracy: 0.6080 - lr: 2.0000e-05\n","Epoch 12/30\n","358/358 [==============================] - ETA: 0s - loss: 0.9109 - accuracy: 0.6639\n","Epoch 12: val_loss did not improve from 1.04276\n","358/358 [==============================] - 1654s 5s/step - loss: 0.9109 - accuracy: 0.6639 - val_loss: 1.0679 - val_accuracy: 0.5995 - lr: 2.0000e-05\n","Epoch 13/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8952 - accuracy: 0.6661\n","Epoch 13: val_loss improved from 1.04276 to 1.03076, saving model to best_emotion_vgg16.h5\n","358/358 [==============================] - 1648s 5s/step - loss: 0.8952 - accuracy: 0.6661 - val_loss: 1.0308 - val_accuracy: 0.6218 - lr: 2.0000e-05\n","Epoch 14/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8827 - accuracy: 0.6762\n","Epoch 14: val_loss did not improve from 1.03076\n","358/358 [==============================] - 1648s 5s/step - loss: 0.8827 - accuracy: 0.6762 - val_loss: 1.0526 - val_accuracy: 0.6096 - lr: 2.0000e-05\n","Epoch 15/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8727 - accuracy: 0.6782\n","Epoch 15: val_loss did not improve from 1.03076\n","358/358 [==============================] - 1664s 5s/step - loss: 0.8727 - accuracy: 0.6782 - val_loss: 1.0322 - val_accuracy: 0.6183 - lr: 2.0000e-05\n","Epoch 16/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8570 - accuracy: 0.6839\n","Epoch 16: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n","\n","Epoch 16: val_loss did not improve from 1.03076\n","358/358 [==============================] - 1638s 5s/step - loss: 0.8570 - accuracy: 0.6839 - val_loss: 1.0584 - val_accuracy: 0.6125 - lr: 2.0000e-05\n","Epoch 17/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8361 - accuracy: 0.6904\n","Epoch 17: val_loss did not improve from 1.03076\n","358/358 [==============================] - 1652s 5s/step - loss: 0.8361 - accuracy: 0.6904 - val_loss: 1.0317 - val_accuracy: 0.6220 - lr: 4.0000e-06\n","Epoch 18/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8302 - accuracy: 0.6915\n","Epoch 18: val_loss improved from 1.03076 to 1.01737, saving model to best_emotion_vgg16.h5\n","358/358 [==============================] - 1650s 5s/step - loss: 0.8302 - accuracy: 0.6915 - val_loss: 1.0174 - val_accuracy: 0.6257 - lr: 4.0000e-06\n","Epoch 19/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8288 - accuracy: 0.6956\n","Epoch 19: val_loss did not improve from 1.01737\n","358/358 [==============================] - 1645s 5s/step - loss: 0.8288 - accuracy: 0.6956 - val_loss: 1.0390 - val_accuracy: 0.6108 - lr: 4.0000e-06\n","Epoch 20/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8221 - accuracy: 0.6987\n","Epoch 20: val_loss did not improve from 1.01737\n","358/358 [==============================] - 1650s 5s/step - loss: 0.8221 - accuracy: 0.6987 - val_loss: 1.0226 - val_accuracy: 0.6192 - lr: 4.0000e-06\n","Epoch 21/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8208 - accuracy: 0.6955\n","Epoch 21: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n","\n","Epoch 21: val_loss did not improve from 1.01737\n","358/358 [==============================] - 1650s 5s/step - loss: 0.8208 - accuracy: 0.6955 - val_loss: 1.0282 - val_accuracy: 0.6229 - lr: 4.0000e-06\n","Epoch 22/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8199 - accuracy: 0.6977\n","Epoch 22: val_loss did not improve from 1.01737\n","358/358 [==============================] - 1649s 5s/step - loss: 0.8199 - accuracy: 0.6977 - val_loss: 1.0343 - val_accuracy: 0.6225 - lr: 8.0000e-07\n","Epoch 23/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8144 - accuracy: 0.7032\n","Epoch 23: val_loss did not improve from 1.01737\n","358/358 [==============================] - 1647s 5s/step - loss: 0.8144 - accuracy: 0.7032 - val_loss: 1.0244 - val_accuracy: 0.6201 - lr: 8.0000e-07\n","Epoch 24/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8127 - accuracy: 0.6987\n","Epoch 24: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n","\n","Epoch 24: val_loss did not improve from 1.01737\n","358/358 [==============================] - 1652s 5s/step - loss: 0.8127 - accuracy: 0.6987 - val_loss: 1.0288 - val_accuracy: 0.6175 - lr: 8.0000e-07\n","Epoch 25/30\n","358/358 [==============================] - ETA: 0s - loss: 0.8105 - accuracy: 0.7022\n","Epoch 25: val_loss did not improve from 1.01737\n","358/358 [==============================] - 1638s 5s/step - loss: 0.8105 - accuracy: 0.7022 - val_loss: 1.0407 - val_accuracy: 0.6227 - lr: 1.6000e-07\n"]}],"source":["# 5. Training callbacks\n","callbacks = [\n","    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True),\n","    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1),\n","    ModelCheckpoint('best_emotion_vgg16.h5', save_best_only=True, monitor='val_loss', verbose=1)\n","]\n","epochs = 30\n","history = model.fit(\n","    train_gen,\n","    steps_per_epoch=steps_train,\n","    validation_data=val_gen,\n","    validation_steps=steps_val,\n","    epochs=epochs,\n","    callbacks=callbacks\n",")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Notes\n","- Training may take significant GPU time.\n","- Will likely increase validation accuracy substantially over base CNN.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":5}
